# Lecture: Introduction to Large Language Models (LLMs) and Generative AI

## Key Concepts

- **Generative AI**: A subset of machine learning where models create content by mimicking human ability, trained on massive datasets.
- **Large Language Models (LLMs)**: Foundation models trained on trillions of words, with billions of parameters, capable of complex tasks like reasoning and problem-solving.

## Use Cases and Interaction

- **Applications**: Chatbots, text-to-image generation, code assistance, and more.
- **Prompt Engineering**: Interacting with LLMs via natural language instructions (prompts) instead of formalized code syntax.
- **Inference**: The process of using LLMs to generate text outputs (completions) based on prompts.

## Model Parameters and Context

- **Parameters**: Think of these as the model's memory; more parameters allow for more sophisticated tasks.
- **Context Window**: The space available for the prompt, typically large enough for a few thousand words, varies by model.

## Practical Implementation

- **Foundation Models**: Like Flan-T5, which can be fine-tuned for specific use cases.
- **Project Lifecycle**: Understanding how to train, fine-tune, and deploy LLMs to solve business or social tasks.

$$
\text{Model Output (Completion)} = \text{Prompt} + \text{Generated Text}
$$

- **Example**: Asking the model where Ganymede is located, it correctly identifies it as a moon of Jupiter.

[[RNNs]]