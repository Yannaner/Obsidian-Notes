# Clustering Algorithms

## Definition
Clustering algorithms are a type of unsupervised learning method used to group similar data points into clusters. The goal is to organize a dataset into subsets or clusters, such that data points within the same cluster are more similar to each other than to those in other clusters.

## Key Concepts
- **Unsupervised Learning**: Unlike supervised learning, clustering does not rely on labeled data. It discovers patterns and structures within the data.
- **Similarity**: The measure of how alike data points are, often defined using distance metrics like Euclidean distance, Manhattan distance, or cosine similarity.
- **Clusters**: Groups of data points that are similar to each other. The number of clusters can be predefined or determined by the algorithm.

## Common Clustering Algorithms
### 1. K-Means Clustering
- **Overview**: Partitions the data into K clusters, where each data point belongs to the cluster with the nearest mean.
- **Steps**:
  1. Initialize K centroids randomly.
  2. Assign each data point to the nearest centroid.
  3. Recalculate the centroids as the mean of all points in a cluster.
  4. Repeat steps 2 and 3 until convergence.
- **Pros**: Simple, efficient for large datasets.
- **Cons**: Requires predefined number of clusters K, sensitive to initial centroid placement.

```
Repeat{
	##Assign points to cluster centroids
	for i=1 to m
	c(1) := index from 1 to K of cluster
		centroid closest to x(i)
	##Move Cluster centroids
	for k = 1 to K
	uK = average of points assigned to cluster k
}
```
- When there are no points assigned to clusters, normally just deleted the cluster
[[K-means Algorithm]]
### 2. Hierarchical Clustering
- **Overview**: Builds a hierarchy of clusters using either an agglomerative (bottom-up) or divisive (top-down) approach.
- **Steps**:
  1. Start with each data point as a separate cluster (agglomerative) or all points in one cluster (divisive).
  2. Merge or split clusters based on a similarity criterion.
  3. Repeat until the desired number of clusters is reached.
- **Pros**: Does not require a predefined number of clusters, produces a dendrogram (tree diagram) that shows the merging or splitting process.
- **Cons**: Computationally intensive for large datasets, less efficient than K-Means.

### 3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
- **Overview**: Groups data points that are closely packed together, marking points in low-density regions as outliers.
- **Steps**:
  1. Choose a point and check its neighborhood using a predefined radius (epsilon).
  2. Form a cluster if the neighborhood has enough points (minPts).
  3. Expand the cluster by including neighboring points that also meet the criteria.
  4. Repeat until all points are classified as cluster members or noise.
- **Pros**: Can find arbitrarily shaped clusters, handles noise well, does not require the number of clusters to be predefined.
- **Cons**: Requires careful tuning of parameters (epsilon and minPts), less effective for clusters with varying densities.

### 4. Gaussian Mixture Models (GMM)
- **Overview**: Assumes data is generated from a mixture of several Gaussian distributions with unknown parameters.
- **Steps**:
  1. Initialize the parameters of the Gaussian distributions.
  2. Use the Expectation-Maximization (EM) algorithm to iteratively refine the parameters.
  3. Assign data points to clusters based on the probability that they were generated by each Gaussian distribution.
- **Pros**: Can model complex cluster shapes, provides a probabilistic assignment of data points to clusters.
- **Cons**: Requires the number of clusters to be predefined, can be computationally intensive.

## Applications
- **Market Segmentation**: Grouping customers with similar behavior for targeted marketing.
- **Image Segmentation**: Partitioning an image into regions for object detection.
- **Anomaly Detection**: Identifying unusual data points that do not fit into any cluster.
- **Document Clustering**: Organizing documents into topics for easier retrieval and analysis.

## Evaluation Metrics
- **Silhouette Score**: Measures how similar a data point is to its own cluster compared to other clusters. Ranges from -1 to 1, with higher values indicating better clustering.
- **Davies-Bouldin Index**: Evaluates the average similarity ratio of each cluster with its most similar cluster. Lower values indicate better clustering.
- **Adjusted Rand Index (ARI)**: Measures the similarity between the true labels and the clustering labels, adjusted for chance. Ranges from -1 to 1, with higher values indicating better clustering.

## Conclusion
Clustering algorithms are essential tools in unsupervised learning, enabling the discovery of patterns and structures in data without the need for labeled examples. Choosing the right clustering algorithm depends on the specific characteristics of the dataset and the desired outcome of the analysis.

## References
- "Pattern Recognition and Machine Learning" by Christopher Bishop
- "Introduction to Data Mining" by Pang-Ning Tan, Michael Steinbach, and Vipin Kumar
- Scikit-learn documentation on clustering algorithms: https://scikit-learn.org/stable/modules/clustering.html
